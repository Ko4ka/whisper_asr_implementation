{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# List of audio file paths (up to 10 files)\n",
    "audio_file_paths = [\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/in-7035555-9587853839-20240802-155950-1722603590.21580.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/out-3036641-3133-20240802-135335-1722596015.21347.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-3101-000047e6-2024-08-02-15-28-09.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-3213-000044b5-2024-08-02-09-34-26.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-3223-0000454f-2024-08-02-10-35-26.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-3353-000048a9-2024-08-02-18-28-28.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-5101-000046c7-2024-08-02-13-00-10.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-5101-0000459d-2024-08-02-11-16-40.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-5101-00004669-2024-08-02-12-27-31.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-5102-000045ea-2024-08-02-11-40-30.wav\"\n",
    "]\n",
    "\n",
    "audio_file_paths = [\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13143_17303__2023_01_31__11_05_42_100.mp3\",\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13301_17384__2023_02_01__17_43_03_327.mp3\",\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13276_17360__2023_02_01__10_11_14_457.mp3\",\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13170_13171__2023_02_01__17_32_20_210.mp3\"\n",
    "]\n",
    "\n",
    "# Step 1: Transcribe the audio files\n",
    "print('Posting audio files for transcription')\n",
    "\n",
    "# Prepare the files for the transcription request\n",
    "transcription_files = [('audio_files', open(file_path, 'rb')) for file_path in audio_file_paths]\n",
    "\n",
    "try:\n",
    "    transcription_response = requests.post(\n",
    "        \"http://localhost:8000/transcribe_mono\",\n",
    "        files=transcription_files\n",
    "    )\n",
    "    transcription_response.raise_for_status()\n",
    "    transcription_data = transcription_response.json()\n",
    "    transcription_results = transcription_data[\"results\"]\n",
    "except Exception as e:\n",
    "    print(f\"Error during transcription: {e}\")\n",
    "    transcription_results = []\n",
    "finally:\n",
    "    # Close the file handles\n",
    "    for _, file_obj in transcription_files:\n",
    "        file_obj.close()\n",
    "\n",
    "print('Posting audio files for transcription -- DONE')\n",
    "\n",
    "# Inspect the transcription results for debugging\n",
    "print(\"Transcription Results:\")\n",
    "print(json.dumps(transcription_results, indent=2))\n",
    "\n",
    "# Step 2: Diarize using the transcription segments\n",
    "print('Posting audio files for diarization')\n",
    "\n",
    "# Prepare the files for the diarization request\n",
    "diarization_files = []\n",
    "transcription_segments_list = []\n",
    "valid_audio_files = []\n",
    "\n",
    "for idx, result in enumerate(transcription_results):\n",
    "    if \"segments\" in result:\n",
    "        # Add the audio file and transcription segments to the lists\n",
    "        file_path = audio_file_paths[idx]\n",
    "        diarization_files.append(('audio_files', open(file_path, 'rb')))\n",
    "        segments_json = json.dumps(result[\"segments\"])\n",
    "        transcription_segments_list.append(segments_json)\n",
    "        valid_audio_files.append(file_path)\n",
    "    else:\n",
    "        # Handle the error case\n",
    "        print(f\"Transcription failed for file: {result.get('file', 'unknown')}\")\n",
    "        print(f\"Error message: {result.get('error', 'No error message available')}\")\n",
    "\n",
    "# Ensure we have valid files to process\n",
    "if not valid_audio_files:\n",
    "    print(\"No valid transcriptions were obtained. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Prepare the data parameter as a list of tuples\n",
    "data = []\n",
    "for segments_json in transcription_segments_list:\n",
    "    data.append(('transcription_segments_list', segments_json))\n",
    "# Add the num_speakers parameter (optional)\n",
    "data.append(('num_speakers', '2'))  # Optional\n",
    "\n",
    "try:\n",
    "    diarization_response = requests.post(\n",
    "        \"http://localhost:8001/diarize\",\n",
    "        files=diarization_files,\n",
    "        data=data\n",
    "    )\n",
    "    diarization_response.raise_for_status()\n",
    "    diarized_data = diarization_response.json()\n",
    "    diarized_results = diarized_data[\"results\"]\n",
    "except Exception as e:\n",
    "    print(f\"Error during diarization: {e}\")\n",
    "    diarized_results = []\n",
    "finally:\n",
    "    # Close the file handles\n",
    "    for _, file_obj in diarization_files:\n",
    "        file_obj.close()\n",
    "\n",
    "print('Posting audio files for diarization -- DONE')\n",
    "\n",
    "# Now you can process 'diarized_results' which contains the diarized segments for each file\n",
    "for diarized_result in diarized_results:\n",
    "    if \"diarized_segments\" in diarized_result:\n",
    "        file_name = diarized_result['file']\n",
    "        diarized_segments = diarized_result['diarized_segments']\n",
    "        print(f\"File: {file_name}\")\n",
    "        for segment in diarized_segments:\n",
    "            start = segment['start']\n",
    "            end = segment['end']\n",
    "            text = segment['text']\n",
    "            speaker = segment['speaker']\n",
    "            print(f\"[{start:.2f} - {end:.2f}] {speaker}: {text}\")\n",
    "        print()\n",
    "    else:\n",
    "        # Handle the error case\n",
    "        f\"Diarization failed for file: {diarized_result.get('file', 'unknown')}\"\n",
    "        f\"Error message: {diarized_result.get('error', 'No error message available')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_paths = [\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13143_17303__2023_01_31__11_05_42_100.mp3\",\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13301_17384__2023_02_01__17_43_03_327.mp3\",\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13276_17360__2023_02_01__10_11_14_457.mp3\",\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13170_13171__2023_02_01__17_32_20_210.mp3\"\n",
    "] \n",
    "\n",
    "stereo_paths = [\n",
    "    'E:/Записи/ФСК/20210623080301062173700pri.wav'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import warnings\n",
    "import torchaudio\n",
    "from tempfile import NamedTemporaryFile\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"whisper\")\n",
    "\n",
    "model = whisper.load_model(\"turbo\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio and pad/trim it to fit 30 seconds\n",
    "audio = whisper.load_audio(audio_file_paths[0])\n",
    "audio = whisper.pad_or_trim(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper.transcribe(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torchaudio way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torchaudio\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tempfile import NamedTemporaryFile\n",
    "import os\n",
    "\n",
    "# Ignore FutureWarnings from whisper\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"whisper\")\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"whisper\")\n",
    "\n",
    "# Load the Whisper model on GPU\n",
    "model = whisper.load_model(\"turbo\", device=\"cuda\")\n",
    "\n",
    "# Path to your stereo audio file\n",
    "stereo_path = 'E:/Записи/ФСК/20210623080301062173700pri.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your stereo audio file\n",
    "stereo_path = 'E:/Записи/ФСК/2021062309023419811401pri.wav'\n",
    "\n",
    "# Load the stereo audio file\n",
    "waveform, sample_rate = torchaudio.load(stereo_path)\n",
    "\n",
    "# Ensure the audio is stereo (2 channels)\n",
    "if waveform.shape[0] == 2:\n",
    "    # Separate channels\n",
    "    channel_0 = waveform[0].unsqueeze(0)  # Channel 0\n",
    "    channel_1 = waveform[1].unsqueeze(0)  # Channel 1\n",
    "\n",
    "    # Save each channel to a temporary file\n",
    "    with NamedTemporaryFile(suffix=\".wav\", delete=False, dir='./temp') as temp_file_0, \\\n",
    "         NamedTemporaryFile(suffix=\".wav\", delete=False, dir='./temp') as temp_file_1:\n",
    "        \n",
    "        # Save channel 0\n",
    "        torchaudio.save(temp_file_0.name, channel_0, sample_rate, encoding=\"PCM_S\", bits_per_sample=16)\n",
    "        channel_0_path = temp_file_0.name\n",
    "        print(f\"Channel 0 saved at: {channel_0_path}\")\n",
    "\n",
    "        # Save channel 1\n",
    "        torchaudio.save(temp_file_1.name, channel_1, sample_rate, encoding=\"PCM_S\", bits_per_sample=16)\n",
    "        channel_1_path = temp_file_1.name\n",
    "        print(f\"Channel 1 saved at: {channel_1_path}\")\n",
    "        #whisper.DecodingOptions()\n",
    "            # Transcribe each channel separately with additional options\n",
    "        print(\"Transcribing Speaker 0...\")\n",
    "        whisper.DecodingOptions()\n",
    "        result_speaker_0 = model.transcribe(\n",
    "            channel_0_path,\n",
    "            language=\"ru\",\n",
    "            initial_prompt='Звонок в компанию, это колл центр застройщика, разговор ведет сотрудник Ольга',\n",
    "            temperature= (0.0, 0.1),\n",
    "            logprob_threshold=-0.6,\n",
    "            no_speech_threshold= 0.0,\n",
    "            compression_ratio_hallucination_threshold=2.1,\n",
    "            condition_on_previous_text=True,\n",
    "            word_timestamps=True,\n",
    "            hallucination_silence_threshold=1\n",
    "        )\n",
    "\n",
    "        print(\"Transcribing Speaker 1...\")\n",
    "        result_speaker_1 = model.transcribe(\n",
    "                channel_1_path,\n",
    "                language=\"ru\",\n",
    "                initial_prompt='Звонок в компанию ФСК, это колл центр застройщика, клиент говорит о покупке квартиры',\n",
    "                temperature=(0.0, 0.1),\n",
    "                no_speech_threshold= 0.0,\n",
    "                condition_on_previous_text=True,\n",
    "                word_timestamps=True,\n",
    "                hallucination_silence_threshold=0.5\n",
    "            )\n",
    "\n",
    "        # Print transcriptions for each speaker\n",
    "        print(\"Transcription for Speaker 0:\")\n",
    "        for segment in result_speaker_0[\"segments\"]:\n",
    "            print(f\"{segment['start']}s - {segment['end']}s: {segment['text']} {segment['compression_ratio']}\")\n",
    "\n",
    "        print(\"\\nTranscription for Speaker 1:\")\n",
    "        for segment in result_speaker_1[\"segments\"]:\n",
    "            print(f\"{segment['start']}s - {segment['end']}s: {segment['text']}\")\n",
    "\n",
    "else:\n",
    "    print(\"Error: Audio is not stereo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_speaker_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunked approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from tempfile import NamedTemporaryFile\n",
    "import os\n",
    "\n",
    "# Path to your stereo audio file\n",
    "stereo_path = 'E:/Записи/ФСК/2021062309023419811401pri.wav'\n",
    "\n",
    "# Load the stereo audio file\n",
    "waveform, sample_rate = torchaudio.load(stereo_path)\n",
    "\n",
    "# Parameters\n",
    "chunk_duration = 900  # Chunk duration in seconds\n",
    "num_channels = waveform.shape[0]\n",
    "chunk_samples = chunk_duration * sample_rate  # Number of samples per chunk\n",
    "\n",
    "# Ensure the audio is stereo (2 channels)\n",
    "if num_channels == 2:\n",
    "    for channel_idx in range(num_channels):\n",
    "        # Select the channel waveform\n",
    "        channel_waveform = waveform[channel_idx].unsqueeze(0)  # Single channel waveform\n",
    "        channel_name = f\"Speaker {channel_idx}\"\n",
    "        \n",
    "        # Split into 90-second chunks\n",
    "        num_chunks = (channel_waveform.shape[1] + chunk_samples - 1) // chunk_samples\n",
    "        transcriptions = []\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start_sample = i * chunk_samples\n",
    "            end_sample = min((i + 1) * chunk_samples, channel_waveform.shape[1])\n",
    "            chunk_waveform = channel_waveform[:, start_sample:end_sample]\n",
    "            start_time = start_sample / sample_rate  # in seconds\n",
    "            end_time = end_sample / sample_rate  # in seconds\n",
    "\n",
    "            # Save each chunk to a temporary file\n",
    "            with NamedTemporaryFile(suffix=\".wav\", delete=False, dir='./temp') as temp_file:\n",
    "                torchaudio.save(temp_file.name, chunk_waveform, sample_rate, encoding=\"PCM_S\", bits_per_sample=16)\n",
    "                temp_path = temp_file.name\n",
    "            \n",
    "            # Transcribe each chunk with original timings\n",
    "            print(f\"Transcribing {channel_name}, chunk {i + 1}/{num_chunks}, from {start_time:.2f}s to {end_time:.2f}s...\")\n",
    "            whisper.DecodingOptions()\n",
    "            result = model.transcribe(\n",
    "                temp_path,\n",
    "                language=\"ru\",\n",
    "                #initial_prompt='Звонок в компанию ФСК, это колл центр застройщика' if channel_idx == 0 else 'Звонок в компанию ФСК, это колл центр застройщика, клиент говорит о покупке квартиры',\n",
    "                temperature=(0.0, 0.1),\n",
    "                no_speech_threshold=0.3,\n",
    "                suppress_tokens = [50365, 2933, 8893, 403, 1635, 10461, 40653, 413, 4775, 51, 284, 89, 453, 51864, 50366, 8567, 1435, 21403, 5627, 15363, 17781, 485, 51863],\n",
    "                condition_on_previous_text=False,\n",
    "                word_timestamps=True,\n",
    "                compression_ratio_hallucination_threshold=2.1,\n",
    "                fp16 = True\n",
    "            )\n",
    "            # Add Субтитры сделал DimaTorzok and other exceptions\n",
    "            # Collect transcriptions with original chunk timing\n",
    "            for segment in result[\"segments\"]:\n",
    "                segment['start'] += start_time\n",
    "                segment['end'] += start_time\n",
    "                transcriptions.append(segment)\n",
    "\n",
    "            # Clean up temporary file\n",
    "            os.remove(temp_path)\n",
    "\n",
    "        # Print transcriptions for the current speaker\n",
    "        print(f\"\\nTranscription for {channel_name}:\")\n",
    "        for segment in transcriptions:\n",
    "            print(f\"{segment['start']}s - {segment['end']}s: {segment['text']}  {segment['compression_ratio']}\")\n",
    "        # print(transcriptions)\n",
    "\n",
    "else:\n",
    "    print(\"Error: Audio is not stereo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load the Whisper model on GPU\n",
    "model = whisper.load_model(\"turbo\", device=\"cuda\")\n",
    "\n",
    "# Path to your stereo audio file\n",
    "stereo_path = 'E:/Записи/ФСК/2021062309023419811401pri.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stereo audio file\n",
    "waveform, sample_rate = torchaudio.load(stereo_path)\n",
    "\n",
    "# Resample the entire waveform to 16000 Hz if necessary\n",
    "target_sample_rate = 16000\n",
    "if sample_rate != target_sample_rate:\n",
    "    waveform = torchaudio.functional.resample(\n",
    "        waveform, sample_rate, target_sample_rate\n",
    "    )\n",
    "    sample_rate = target_sample_rate\n",
    "\n",
    "# Parameters\n",
    "chunk_duration = 900  # Chunk duration in seconds\n",
    "num_channels = waveform.shape[0]\n",
    "chunk_samples = int(chunk_duration * sample_rate)  # Number of samples per chunk\n",
    "\n",
    "# Ensure the audio is stereo (2 channels)\n",
    "if num_channels == 2:\n",
    "    for channel_idx in range(num_channels):\n",
    "        # Select the channel waveform\n",
    "        channel_waveform = waveform[channel_idx].unsqueeze(0)  # Shape: [1, num_samples]\n",
    "        channel_name = f\"Speaker {channel_idx}\"\n",
    "\n",
    "        # Split into chunks\n",
    "        num_samples = channel_waveform.shape[1]\n",
    "        num_chunks = (num_samples + chunk_samples - 1) // chunk_samples\n",
    "        transcriptions = []\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start_sample = i * chunk_samples\n",
    "            end_sample = min((i + 1) * chunk_samples, num_samples)\n",
    "            chunk_waveform = channel_waveform[:, start_sample:end_sample]\n",
    "            start_time = start_sample / sample_rate  # in seconds\n",
    "\n",
    "            # Convert chunk_waveform to NumPy array\n",
    "            chunk_numpy = chunk_waveform.squeeze().numpy()\n",
    "\n",
    "            # Transcribe the chunk\n",
    "            print(\n",
    "                f\"Transcribing {channel_name}, chunk {i + 1}/{num_chunks}, from {start_time:.2f}s...\"\n",
    "            )\n",
    "\n",
    "            result = model.transcribe(\n",
    "                audio=chunk_numpy,\n",
    "                language=\"ru\",\n",
    "                temperature=(0.0, 0.1),\n",
    "                no_speech_threshold=0.3,\n",
    "                suppress_tokens=[\n",
    "                    50365, 2933, 8893, 403, 1635, 10461, 40653,\n",
    "                    413, 4775, 51, 284, 89, 453, 51864, 50366,\n",
    "                    8567, 1435, 21403, 5627, 15363, 17781, 485,\n",
    "                    51863\n",
    "                ],\n",
    "                condition_on_previous_text=False,\n",
    "                word_timestamps=True,\n",
    "                compression_ratio_hallucination_threshold=2.1,\n",
    "                fp16=True,\n",
    "            )\n",
    "\n",
    "            # Adjust the segment times\n",
    "            for segment in result[\"segments\"]:\n",
    "                segment['start'] += start_time\n",
    "                segment['end'] += start_time\n",
    "                transcriptions.append(segment)\n",
    "\n",
    "        # Print transcriptions for the current speaker\n",
    "        print(f\"\\nTranscription for {channel_name}:\")\n",
    "        for segment in transcriptions:\n",
    "            print(\n",
    "                f\"{segment['start']:.2f}s - {segment['end']:.2f}s: {segment['text']}  {segment.get('compression_ratio', '')}\"\n",
    "            )\n",
    "\n",
    "else:\n",
    "    print(\"Error: Audio is not stereo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
    "vad = load_silero_vad()\n",
    "wav = read_audio('c:/Users/Alex/whisper_asr_implementation/Drafts/temp/tmpbshce7ez.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_timestamps = get_speech_timestamps(\n",
    "  wav,\n",
    "  vad,\n",
    "  min_speech_duration_ms=400,\n",
    "  return_seconds=True,  # Return speech timestamps in seconds (default is samples)\n",
    "  sampling_rate=16000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge VAD segments that are within 0.5 seconds of each other\n",
    "def merge_vad_segments(vad_segments, merge_threshold=0.5):\n",
    "    if not vad_segments:\n",
    "        return []\n",
    "\n",
    "    merged_segments = []\n",
    "    current_segment = vad_segments[0]\n",
    "\n",
    "    for next_segment in vad_segments[1:]:\n",
    "        if next_segment['start'] - current_segment['end'] <= merge_threshold:\n",
    "            # Extend the current segment's end time\n",
    "            current_segment['end'] = next_segment['end']\n",
    "        else:\n",
    "            merged_segments.append(current_segment)\n",
    "            current_segment = next_segment\n",
    "    merged_segments.append(current_segment)\n",
    "\n",
    "    return merged_segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load('c:/Users/Alex/whisper_asr_implementation/Drafts/temp/tmpbshce7ez.wav')\n",
    "num_channels = waveform.shape[0]\n",
    "vad_output = speech_timestamps\n",
    "# Apply merging\n",
    "vad_output = merge_vad_segments(vad_output, merge_threshold=2)\n",
    "# Process each channel separately\n",
    "for channel_idx in range(num_channels):\n",
    "    channel_waveform = waveform[channel_idx]  # Single channel waveform\n",
    "    channel_name = f\"Speaker {channel_idx}\"\n",
    "\n",
    "    transcriptions = []\n",
    "\n",
    "    # Process each VAD segment\n",
    "    for i, vad_segment in enumerate(vad_output):\n",
    "        start_time = vad_segment['start']\n",
    "        end_time = vad_segment['end']\n",
    "        start_sample = int(start_time * sample_rate)\n",
    "        end_sample = int(end_time * sample_rate)\n",
    "\n",
    "        # Extract the audio segment\n",
    "        segment_waveform = channel_waveform[start_sample:end_sample]\n",
    "\n",
    "        # Check if the segment is non-empty\n",
    "        if segment_waveform.numel() == 0:\n",
    "            continue  # Skip empty segments\n",
    "\n",
    "        # Reshape to (1, N) for a single channel\n",
    "        segment_waveform = segment_waveform.unsqueeze(0)\n",
    "\n",
    "        # Resample to 16 kHz if necessary\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            segment_waveform = resampler(segment_waveform)\n",
    "\n",
    "        # Convert to numpy array as Whisper expects NumPy arrays\n",
    "        segment_waveform_np = segment_waveform.squeeze(0).numpy()\n",
    "\n",
    "        # Transcribe the segment\n",
    "        print(f\"Transcribing {channel_name}, segment {i + 1}/{len(vad_output)}, from {start_time:.2f}s to {end_time:.2f}s...\")\n",
    "\n",
    "        result = model.transcribe(\n",
    "            audio=segment_waveform_np,\n",
    "            language=\"ru\",\n",
    "            initial_prompt='Звонок в компанию ФСК, это колл центр застройщика',\n",
    "            temperature=(0.0, 0.1),\n",
    "            no_speech_threshold=0.6,\n",
    "            condition_on_previous_text=True,\n",
    "            word_timestamps=True,\n",
    "            hallucination_silence_threshold=0.1 \n",
    "        )\n",
    "\n",
    "        # Collect transcriptions with original timings\n",
    "        for segment in result[\"segments\"]:\n",
    "            # Adjust the timestamps to the original audio timeline\n",
    "            segment['start'] += start_time\n",
    "            segment['end'] += start_time\n",
    "            transcriptions.append(segment)\n",
    "\n",
    "    # Print transcriptions for the current speaker\n",
    "    print(f\"\\nTranscription for {channel_name}:\")\n",
    "    for segment in transcriptions:\n",
    "        print(f\"{segment['start']:.2f}s - {segment['end']:.2f}s: {segment['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = whisper.load_audio('c:/Users/Alex/whisper_asr_implementation/Drafts/temp/tmpbshce7ez.wav')\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio, n_mels=128).to(model.device)\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions(language='ru')\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "result.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/pull/28556/files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3-turbo\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Enable static cache and compile the forward pass\n",
    "model.generation_config.max_new_tokens = 256\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "generate_kwargs = {\n",
    "    \"condition_on_prev_tokens\": True,\n",
    "    \"temperature\": (0.0, 0.2, 0.4),\n",
    "    \"logprob_threshold\": -0.4,\n",
    "    \"no_speech_threshold\": 0.05,\n",
    "    \"return_timestamps\": \"word\",\n",
    "    #\"task\": \"transcribe\",\n",
    "    \"language\": \"russian\",\n",
    "    #\"initial_prompt\": \"ФСК\"  # https://github.com/huggingface/transformers/issues/27317\n",
    "    \n",
    "}\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s=30,  # When no is passed - sliding window\n",
    "    batch_size=32,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    generate_kwargs=generate_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe('c:/Users/Alex/whisper_asr_implementation/Drafts/temp/tmpxf4eqn1b.wav', return_timestamps=True)\n",
    "\n",
    "for chunk in result[\"chunks\"]:\n",
    "    print(str(chunk[\"timestamp\"]) + '  ' + chunk['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt 31-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of audio file paths (up to 10 files)\n",
    "audio_file_paths = [\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/in-7035555-9587853839-20240802-155950-1722603590.21580.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/out-3036641-3133-20240802-135335-1722596015.21347.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-3101-000047e6-2024-08-02-15-28-09.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-3213-000044b5-2024-08-02-09-34-26.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-3223-0000454f-2024-08-02-10-35-26.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-3353-000048a9-2024-08-02-18-28-28.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-5101-000046c7-2024-08-02-13-00-10.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-5101-0000459d-2024-08-02-11-16-40.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-5101-00004669-2024-08-02-12-27-31.wav\",\n",
    "    \"E:/Записи/ФСК СЗ/incom/08/02/PJSIP-5102-000045ea-2024-08-02-11-40-30.wav\"\n",
    "]\n",
    "\n",
    "'''audio_file_paths = [\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13143_17303__2023_01_31__11_05_42_100.mp3\",\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13301_17384__2023_02_01__17_43_03_327.mp3\",\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13276_17360__2023_02_01__10_11_14_457.mp3\",\n",
    "    \"E:/Записи/BorAvto/ОП/mix_13170_13171__2023_02_01__17_32_20_210.mp3\"\n",
    "]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import librosa\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"turbo\")\n",
    "stereo_path = 'E:/Записи/ФСК/2021062309023419811401pri.wav'\n",
    "# Load the stereo audio file with librosa\n",
    "audio, sr = librosa.load(stereo_path, sr=8000, mono=False)\n",
    "\n",
    "# Ensure the audio has two channels\n",
    "if audio.shape[0] != 2:\n",
    "    raise ValueError(\"Audio file does not have two channels.\")\n",
    "\n",
    "# Separate the left and right channels\n",
    "audio_left = audio[0]\n",
    "audio_right = audio[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the left channel\n",
    "#audio_left = whisper.pad_or_trim(audio_left)\n",
    "\n",
    "# Transcribe the left channel\n",
    "result_left = model.transcribe(\n",
    "    audio_left,\n",
    "    verbose=True,\n",
    "    language='ru'\n",
    ")\n",
    "\n",
    "#print(\"Left Channel Transcription:\")\n",
    "#print(result_left)\n",
    "\n",
    "'''# Process the right channel\n",
    "audio_right = whisper.pad_or_trim(audio_right)\n",
    "mel_right = whisper.log_mel_spectrogram(audio_right, n_mels=128).to(model.device)\n",
    "\n",
    "# Transcribe the right channel using the same options\n",
    "result_right = whisper.decode(model, mel_right, options)\n",
    "print(\"Right Channel Transcription:\")'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAD approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import librosa\n",
    "import numpy as np\n",
    "import webrtcvad\n",
    "import collections\n",
    "\n",
    "# Step 1: Load and Resample the Audio File\n",
    "audio_path = 'E:/Записи/ФСК/2021062309023419811401pri.wav'\n",
    "audio_data, sr = librosa.load(audio_path, sr=8000, mono=False)  # Keep stereo channels\n",
    "\n",
    "# Resample to 16 kHz\n",
    "audio_data_16k = librosa.resample(audio_data, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "# Step 2: Split into Left and Right Channels\n",
    "left_channel = audio_data_16k[0, :]\n",
    "right_channel = audio_data_16k[1, :]\n",
    "\n",
    "channels = [left_channel, right_channel]\n",
    "\n",
    "# Load the Whisper Model\n",
    "model = whisper.load_model(\"turbo\")\n",
    "\n",
    "# Step 3: Define VAD Functions\n",
    "vad = webrtcvad.Vad(2)  # Aggressiveness mode (0-3)\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "    frame_length = int(sample_rate * frame_duration_ms / 1000)\n",
    "    num_frames = len(audio) // frame_length\n",
    "    for i in range(num_frames):\n",
    "        yield audio[i * frame_length:(i + 1) * frame_length]\n",
    "\n",
    "def vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, audio):\n",
    "    frames = list(frame_generator(frame_duration_ms, audio, sample_rate))\n",
    "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
    "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "    triggered = False\n",
    "    voiced_frames = []\n",
    "    segments = []\n",
    "\n",
    "    for frame in frames:\n",
    "        # Convert to 16-bit PCM\n",
    "        pcm_frame = (frame * 32767).astype(np.int16).tobytes()\n",
    "        is_speech = vad.is_speech(pcm_frame, sample_rate)\n",
    "\n",
    "        if not triggered:\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = True\n",
    "                voiced_frames.extend([f for f, s in ring_buffer])\n",
    "                ring_buffer.clear()\n",
    "        else:\n",
    "            voiced_frames.append(frame)\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = False\n",
    "                segments.append(np.concatenate(voiced_frames))\n",
    "                ring_buffer.clear()\n",
    "                voiced_frames = []\n",
    "    if voiced_frames:\n",
    "        segments.append(np.concatenate(voiced_frames))\n",
    "    return segments\n",
    "\n",
    "# Step 4: Transcribe Each Channel\n",
    "sample_rate = 16000\n",
    "frame_duration_ms = 30\n",
    "padding_duration_ms = 300\n",
    "\n",
    "for idx, ch in enumerate(channels):\n",
    "    # Apply VAD\n",
    "    speech_segments = vad_collector(sample_rate, frame_duration_ms, padding_duration_ms, vad, ch)\n",
    "    print(f\"Processing Channel {idx + 1}\")\n",
    "    for i, segment in enumerate(speech_segments):\n",
    "        # Transcribe with Whisper\n",
    "        result = model.transcribe(segment, language='ru', fp16=False)\n",
    "        print(f\"Segment {i + 1}: {result['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "import os\n",
    "\n",
    "# Diarization Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token='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'  # Replace with your Hugging Face token\n",
    ")\n",
    "pipeline.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"E:/Записи/Мигкредит/1/1_Ивченко Д.А_2018-12-10_15-50-35_6136_89055400861_4H7P4LS4QL76B4U1JRH8R0HRAG000939_pcmu.wav\",\n",
    "           \"E:/Записи/Мигкредит/1/9_Стриж А_2018-12-10_09-23-21_6127_89033605019_IBA75L9EK10JJ452OB2KJ0H51K007DO2_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2016-11-22_06-30-11_6136_89518759355_6IMC8TIPUP6BD8RC4L2T32JC6O07SK0M_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2016-11-22_06-32-02_6136_89518759355_6IMC8TIPUP6BD8RC4L2T32JC6O07SKAU_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2017-12-29_07-52-31_6130_89303431192_Садоев К.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-03-18_15-07-01_6132_89507086469_Перелыгин И.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-03-23_05-35-26_6135_89025322333_38NDLA8KCD09R2FR3R9CO3JCT4017CD2_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-03-23_05-37-29_6135_89025322333_38NDLA8KCD09R2FR3R9CO3JCT4017CE9_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-03-26_11-09-32_6171_89610426801_Гспоян М.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-03-29_10-00-38_6001_89535554036_Автаев А.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-06-06_12-52-10_6134_88462604792_3RKH6IPB751TR09NAMG5QSSGJK01CHLB_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-06-12_04-01-07_6001_89824570468_3RKH6IPB751TR09NAMG5QSSGJK01PAC9_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-06-18_10-00-20_6140_89677413899_R4P7NOLATT55TD0ANNFK8URBGG00R4AM_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-06-18_13-00-21_6129_89123448698_R4P7NOLATT55TD0ANNFK8URBGG00RJE2_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-07-04_09-14-52_Resources_89655729177_BJSQGAOFE15KN5QPPG58M763MC00KMBI_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-08-14_08-11-05_Resources_89103996696_9L22UTJ9DH4TT09F3KID43KHUS00L1LC_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-09-20_14-00-24_6133_89993680214_FTHN9PABGL6PN0RKA92QRCNI5S00V767_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-09-20_14-07-23_6133_89993680214_FTHN9PABGL6PN0RKA92QRCNI5S00V81R_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-10-17_06-27-29_4992679575_4997023679_38CBN0BE1569T2FAUHMUAT12TC00CV1V_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2018-10-20_14-17-38_6130_89260446126_38CBN0BE1569T2FAUHMUAT12TC00JKQO_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2022-02-23_05-15-49_6106_89134343721_0J8UE9S7N91VL6BMQFUNONT6VS03FMR4_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2022-04-08_08-35-58_6105_89113208278_DCM56DJNV90PR2IQHNAON84VMK00RUAA_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2022-04-11_07-29-22_6105_89113208278_DCM56DJNV90PR2IQHNAON84VMK0128T2_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2022-11-16_12-40-48_Resources_89204089388_1IE7MJESO941555IRFPUOP8EJO0DO534_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2023-01-02_12-31-54_6104_89652214529_1IE7MJESO941555IRFPUOP8EJO0I5MCA_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2023-01-02_12-53-54_6104_89103617383_1IE7MJESO941555IRFPUOP8EJO0I5MD1_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2023-01-02_13-06-43_9111471194_4997023679_1IE7MJESO941555IRFPUOP8EJO0I5MDD_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2023-02-27_05-35-48_Resources_89035610477_TRG4D987PP3P3AE4SA59SAUVVK083UAU_pcmu.wav\",\n",
    "\"E:/Записи/Мигкредит/1/2023-03-07_04-14-56_Resources_89122646096_TRG4D987PP3P3AE4SA59SAUVVK092JNT_pcmu.wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline(dataset[i], num_speakers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = pipeline(dataset[i], num_speakers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/transcribe_audio_bulk\"\n",
    "\n",
    "file_name = '2023-09-15_15-35-23_Resources_89688627131_TRG4D987PP3P3AE4SA59SAUVVK14OK43_pcmu.wav'\n",
    "file_path = f'E:/Записи/Мигкредит/1/{file_name}'\n",
    "\n",
    "payload = {}\n",
    "files=[\n",
    "  ('files',(file_name,open(file_path,'rb'),'audio/wav'))\n",
    "]\n",
    "headers = {}\n",
    "\n",
    "transcription = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "\n",
    "\n",
    "url = \"http://127.0.0.1:8001/diarize_audio_bulk\"\n",
    "\n",
    "payload = {'num_speakers': '2'}\n",
    "files=[\n",
    "  ('files',(file_name,open(file_path,'rb'),'audio/wav'))\n",
    "]\n",
    "headers = {}\n",
    "\n",
    "segments = requests.request(\"POST\", url, headers=headers, data=payload, files=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = segments.json()\n",
    "transcription = transcription.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_transcription_with_diarization(transcription, diarization, overlap_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Aligns transcription words with diarization segments, ensuring each word is\n",
    "    aligned with all segments it significantly overlaps with.\n",
    "    \"\"\"\n",
    "    # Flatten the word list from the transcription data\n",
    "    words = []\n",
    "    for speaker, segments in transcription.items():\n",
    "        for segment in segments:\n",
    "            words.extend(segment['words'])\n",
    "    \n",
    "    aligned_words = []\n",
    "    \n",
    "    # Loop over each word to align it with overlapping diarization segments\n",
    "    for word in words:\n",
    "        word_start = word['start']\n",
    "        word_end = word['end']\n",
    "        word_duration = word_end - word_start\n",
    "        word_text = word['word']\n",
    "    \n",
    "        # Keep track of overlaps with each speaker\n",
    "        overlaps = []\n",
    "    \n",
    "        # Compare the word against all diarization segments\n",
    "        for diarization_segment in diarization:\n",
    "            segment_start = diarization_segment['start']\n",
    "            segment_end = diarization_segment['end']\n",
    "            segment_speaker = diarization_segment['speaker']\n",
    "    \n",
    "            # Calculate overlap\n",
    "            overlap_start = max(word_start, segment_start)\n",
    "            overlap_end = min(word_end, segment_end)\n",
    "            overlap_duration = max(0, overlap_end - overlap_start)\n",
    "    \n",
    "            # Calculate overlap percentage\n",
    "            overlap_percentage = overlap_duration / word_duration if word_duration > 0 else 0\n",
    "    \n",
    "            # Assign word to speaker if overlap is significant\n",
    "            if overlap_percentage >= overlap_threshold:\n",
    "                overlaps.append(segment_speaker)\n",
    "    \n",
    "        # Append aligned words for all overlapping speakers\n",
    "        for speaker in overlaps:\n",
    "            aligned_word = {\n",
    "                'word': word_text,\n",
    "                'start': word_start,\n",
    "                'end': word_end,\n",
    "                'speaker': speaker\n",
    "            }\n",
    "            aligned_words.append(aligned_word)\n",
    "    \n",
    "    return aligned_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = align_transcription_with_diarization(\n",
    "transcription[file_name],\n",
    "segments[file_name]['diarization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_speech_bubbles(transcription, pause_threshold=0.5, max_duration=5.0):\n",
    "    speech_bubbles = []\n",
    "    speaker_bubbles = {}  # Holds the current bubble for each speaker\n",
    "    last_end_times = {}   # Tracks the last end time for each speaker\n",
    "\n",
    "    # Ensure transcription is sorted by start time\n",
    "    transcription.sort(key=lambda x: x['start'])\n",
    "\n",
    "    for word_data in transcription:\n",
    "        word = word_data['word']\n",
    "        start_time = word_data['start']\n",
    "        end_time = word_data['end']\n",
    "        speaker = word_data['speaker']\n",
    "\n",
    "        # Initialize the current bubble for the speaker if not already present\n",
    "        if speaker not in speaker_bubbles:\n",
    "            speaker_bubbles[speaker] = {\"speaker\": speaker, \"start\": None, \"end\": None, \"text\": \"\", \"overlap\": False}\n",
    "            last_end_times[speaker] = None\n",
    "\n",
    "        current_bubble = speaker_bubbles[speaker]\n",
    "        last_end_time = last_end_times[speaker]\n",
    "\n",
    "        # If the current bubble is empty, initialize it with the current word\n",
    "        if current_bubble[\"start\"] is None:\n",
    "            current_bubble[\"start\"] = start_time\n",
    "            current_bubble[\"end\"] = end_time\n",
    "            current_bubble[\"text\"] = word\n",
    "        else:\n",
    "            # Check if we need to start a new bubble\n",
    "            has_long_pause = last_end_time and (start_time - last_end_time > pause_threshold)\n",
    "            exceeds_max_duration = (end_time - current_bubble[\"start\"]) > max_duration\n",
    "\n",
    "            if has_long_pause or exceeds_max_duration:\n",
    "                # Finalize the current bubble and start a new one\n",
    "                speech_bubbles.append(current_bubble)\n",
    "                speaker_bubbles[speaker] = {\n",
    "                    \"speaker\": speaker,\n",
    "                    \"start\": start_time,\n",
    "                    \"end\": end_time,\n",
    "                    \"text\": word,\n",
    "                    \"overlap\": False\n",
    "                }\n",
    "                current_bubble = speaker_bubbles[speaker]\n",
    "            else:\n",
    "                # Continue the current bubble\n",
    "                current_bubble[\"text\"] += \" \" + word\n",
    "                current_bubble[\"end\"] = end_time\n",
    "\n",
    "        # Update the last end time for the speaker\n",
    "        last_end_times[speaker] = end_time\n",
    "\n",
    "    # Append any remaining bubbles\n",
    "    for bubble in speaker_bubbles.values():\n",
    "        if bubble[\"start\"] is not None:\n",
    "            speech_bubbles.append(bubble)\n",
    "\n",
    "    # Now, sort the bubbles by start time\n",
    "    speech_bubbles.sort(key=lambda x: x['start'])\n",
    "\n",
    "    # Detect overlaps between bubbles of different speakers and set 'overlap': True\n",
    "    for i in range(len(speech_bubbles)):\n",
    "        bubble_i = speech_bubbles[i]\n",
    "        for j in range(i + 1, len(speech_bubbles)):\n",
    "            bubble_j = speech_bubbles[j]\n",
    "            # Stop checking if the next bubble starts after the current bubble ends\n",
    "            if bubble_j['start'] > bubble_i['end']:\n",
    "                break\n",
    "            # Check if bubbles are from different speakers and overlap\n",
    "            if bubble_i['speaker'] != bubble_j['speaker']:\n",
    "                # Check for overlap\n",
    "                start_i, end_i = bubble_i['start'], bubble_i['end']\n",
    "                start_j, end_j = bubble_j['start'], bubble_j['end']\n",
    "                # Overlap exists if start_i < end_j and start_j < end_i\n",
    "                if start_i < end_j and start_j < end_i:\n",
    "                    # Set 'overlap': True in both bubbles\n",
    "                    bubble_i['overlap'] = True\n",
    "                    bubble_j['overlap'] = True\n",
    "\n",
    "    return speech_bubbles\n",
    "\n",
    "def generate_html_with_media_player(speech_bubbles, audio_file_url, output_filename=\"transcription_with_player.html\"):\n",
    "    # Define the HTML structure with Plyr.js for the media player\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>Speech Bubbles with Media Player</title>\n",
    "        <link rel=\"stylesheet\" href=\"https://cdn.plyr.io/3.7.8/plyr.css\" />\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                background-color: #f4f4f9;\n",
    "                color: #333;\n",
    "                padding: 20px;\n",
    "                margin: 0;\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "                align-items:center;\n",
    "            }}\n",
    "            .sticky-player {{\n",
    "                position: fixed;\n",
    "                top: 10px;\n",
    "                left: 50%;\n",
    "                transform: translateX(-50%);\n",
    "                z-index: 1000;\n",
    "                width: 90%;\n",
    "                max-width: 600px;\n",
    "                background-color: white;\n",
    "                border: 1px solid #ccc;\n",
    "                border-radius: 10px;\n",
    "                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
    "                padding: 10px;\n",
    "            }}\n",
    "            .bubble-container {{\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "                gap: 10px;\n",
    "                margin-top: 120px; /* To avoid overlapping with the fixed player */\n",
    "            }}\n",
    "            .bubble {{\n",
    "                border-radius: 10px;\n",
    "                padding: 15px;\n",
    "                max-width: 70%;\n",
    "                word-wrap: break-word;\n",
    "            }}\n",
    "            .bubble.speaker-0 {{\n",
    "                background-color: #d1e7ff;\n",
    "                color: #0a58ca;\n",
    "                align-self: flex-start;\n",
    "            }}\n",
    "            .bubble.speaker-1 {{\n",
    "                background-color: #ffe0e0;\n",
    "                color: #c92a2a;\n",
    "                align-self: flex-end;\n",
    "            }}\n",
    "            .timestamp {{\n",
    "                font-size: 0.85em;\n",
    "                color: #555;\n",
    "                margin-top: 5px;\n",
    "                text-align: right;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"sticky-player\">\n",
    "            <audio id=\"player\" controls>\n",
    "                <source src=\"{audio_file_url}\" type=\"audio/mpeg\">\n",
    "                Your browser does not support the audio element.\n",
    "            </audio>\n",
    "        </div>\n",
    "        <div class=\"bubble-container\">\n",
    "    \"\"\"\n",
    "\n",
    "    # Add bubbles for each speech segment\n",
    "    for bubble in speech_bubbles:\n",
    "        speaker_class = \"speaker-0\" if bubble[\"speaker\"] == \"SPEAKER_00\" else \"speaker-1\"\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"bubble {speaker_class}\">\n",
    "            <div class=\"text\">{bubble[\"text\"]}</div>\n",
    "            <div class=\"timestamp\">[{bubble[\"start\"]:.2f} - {bubble[\"end\"]:.2f}]</div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    # Close the HTML structure\n",
    "    html_content += \"\"\"\n",
    "        </div>\n",
    "        <script src=\"https://cdn.plyr.io/3.7.8/plyr.polyfilled.js\"></script>\n",
    "        <script>\n",
    "            const player = new Plyr('#player', {\n",
    "                controls: ['play', 'progress', 'current-time', 'duration', 'mute', 'volume']\n",
    "            });\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Write to an HTML file\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML file with media player has been generated: {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubbles = create_speech_bubbles(test)\n",
    "\n",
    "generate_html_with_media_player(bubbles, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom overlapping approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_speech_bubbles_t(transcription, pause_threshold=0.5, max_duration=5.0):\n",
    "    speech_bubbles = []\n",
    "    speaker_bubbles = {}  # Holds the current bubble for each speaker\n",
    "    last_end_times = {}   # Tracks the last end time for each speaker\n",
    "\n",
    "    # Ensure transcription is sorted by start time\n",
    "    transcription.sort(key=lambda x: x['start'])\n",
    "\n",
    "    for word_data in transcription:\n",
    "        word = word_data['word']\n",
    "        start_time = word_data['start']\n",
    "        end_time = word_data['end']\n",
    "        speaker = word_data['speaker']\n",
    "\n",
    "        # Initialize the current bubble for the speaker if not already present\n",
    "        if speaker not in speaker_bubbles:\n",
    "            speaker_bubbles[speaker] = {\"speaker\": speaker, \"start\": None, \"end\": None, \"text\": \"\", \"overlap\": \"\"}\n",
    "            last_end_times[speaker] = None\n",
    "\n",
    "        current_bubble = speaker_bubbles[speaker]\n",
    "        last_end_time = last_end_times[speaker]\n",
    "\n",
    "        # If the current bubble is empty, initialize it with the current word\n",
    "        if current_bubble[\"start\"] is None:\n",
    "            current_bubble[\"start\"] = start_time\n",
    "            current_bubble[\"end\"] = end_time\n",
    "            current_bubble[\"text\"] = word\n",
    "        else:\n",
    "            # Check if we need to start a new bubble\n",
    "            has_long_pause = last_end_time and (start_time - last_end_time > pause_threshold)\n",
    "            exceeds_max_duration = (end_time - current_bubble[\"start\"]) > max_duration\n",
    "\n",
    "            if has_long_pause or exceeds_max_duration:\n",
    "                # Finalize the current bubble and start a new one\n",
    "                speech_bubbles.append(current_bubble)\n",
    "                speaker_bubbles[speaker] = {\n",
    "                    \"speaker\": speaker,\n",
    "                    \"start\": start_time,\n",
    "                    \"end\": end_time,\n",
    "                    \"text\": word,\n",
    "                    \"overlap\": \"\"\n",
    "                }\n",
    "                current_bubble = speaker_bubbles[speaker]\n",
    "            else:\n",
    "                # Continue the current bubble\n",
    "                current_bubble[\"text\"] += \"\" + word\n",
    "                current_bubble[\"end\"] = end_time\n",
    "\n",
    "        # Update the last end time for the speaker\n",
    "        last_end_times[speaker] = end_time\n",
    "\n",
    "    # Append any remaining bubbles\n",
    "    for bubble in speaker_bubbles.values():\n",
    "        if bubble[\"start\"] is not None:\n",
    "            speech_bubbles.append(bubble)\n",
    "\n",
    "    # Sort the bubbles by start time\n",
    "    speech_bubbles.sort(key=lambda x: x['start'])\n",
    "\n",
    "    # Detect overlaps between bubbles of different speakers and capture exact sequences\n",
    "    for i in range(len(speech_bubbles)):\n",
    "        bubble_i = speech_bubbles[i]\n",
    "        for j in range(i + 1, len(speech_bubbles)):\n",
    "            bubble_j = speech_bubbles[j]\n",
    "            # Stop checking if the next bubble starts after the current bubble ends\n",
    "            if bubble_j['start'] > bubble_i['end']:\n",
    "                break\n",
    "            # Check if bubbles are from different speakers and overlap\n",
    "            if bubble_i['speaker'] != bubble_j['speaker']:\n",
    "                # Check for overlap\n",
    "                start_i, end_i = bubble_i['start'], bubble_i['end']\n",
    "                start_j, end_j = bubble_j['start'], bubble_j['end']\n",
    "                if start_i < end_j and start_j < end_i:\n",
    "                    # Identify exact overlapping sequences\n",
    "                    words_i = bubble_i['text'].split()\n",
    "                    words_j = bubble_j['text'].split()\n",
    "                    overlap_sequence = []\n",
    "\n",
    "                    # Compare sequences of words\n",
    "                    for idx_i, word_i in enumerate(words_i):\n",
    "                        for idx_j, word_j in enumerate(words_j):\n",
    "                            if word_i == word_j:\n",
    "                                temp_sequence = []\n",
    "                                k = 0\n",
    "                                # Check for a sequence match\n",
    "                                while (\n",
    "                                    idx_i + k < len(words_i)\n",
    "                                    and idx_j + k < len(words_j)\n",
    "                                    and words_i[idx_i + k] == words_j[idx_j + k]\n",
    "                                ):\n",
    "                                    temp_sequence.append(words_i[idx_i + k])\n",
    "                                    k += 1\n",
    "                                if len(temp_sequence) > len(overlap_sequence):\n",
    "                                    overlap_sequence = temp_sequence\n",
    "\n",
    "                    if overlap_sequence:\n",
    "                        overlap_text = \" \".join(overlap_sequence)\n",
    "                        bubble_i['overlap'] = overlap_text\n",
    "                        bubble_j['overlap'] = overlap_text\n",
    "\n",
    "    return speech_bubbles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_html_with_media_player_t(speech_bubbles, audio_file_url, output_filename=\"transcription_with_player.html\"):\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>Speech Bubbles with Media Player</title>\n",
    "        <link rel=\"stylesheet\" href=\"https://cdn.plyr.io/3.7.8/plyr.css\" />\n",
    "        <style>\n",
    "            body {{\n",
    "                font-family: Arial, sans-serif;\n",
    "                background-color: #f4f4f9;\n",
    "                color: #333;\n",
    "                padding: 20px;\n",
    "                margin: 0;\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "                align-items:center;\n",
    "            }}\n",
    "            .sticky-player {{\n",
    "                position: fixed;\n",
    "                top: 10px;\n",
    "                left: 50%;\n",
    "                transform: translateX(-50%);\n",
    "                z-index: 1000;\n",
    "                width: 90%;\n",
    "                max-width: 600px;\n",
    "                background-color: white;\n",
    "                border: 1px solid #ccc;\n",
    "                border-radius: 10px;\n",
    "                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
    "                padding: 10px;\n",
    "            }}\n",
    "            .bubble-container {{\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "                gap: 10px;\n",
    "                margin-top: 120px;\n",
    "            }}\n",
    "            .bubble {{\n",
    "                border-radius: 10px;\n",
    "                padding: 15px;\n",
    "                max-width: 70%;\n",
    "                word-wrap: break-word;\n",
    "            }}\n",
    "            .bubble.speaker-0 {{\n",
    "                background-color: #d1e7ff;\n",
    "                color: #0a58ca;\n",
    "                align-self: flex-start;\n",
    "            }}\n",
    "            .bubble.speaker-1 {{\n",
    "                background-color: #ffe0e0;\n",
    "                color: #c92a2a;\n",
    "                align-self: flex-end;\n",
    "            }}\n",
    "            .overlap-word {{\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "            .timestamp {{\n",
    "                font-size: 0.85em;\n",
    "                color: #555;\n",
    "                margin-top: 5px;\n",
    "                text-align: right;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"sticky-player\">\n",
    "            <audio id=\"player\" controls>\n",
    "                <source src=\"{audio_file_url}\" type=\"audio/mpeg\">\n",
    "                Your browser does not support the audio element.\n",
    "            </audio>\n",
    "        </div>\n",
    "        <div class=\"bubble-container\">\n",
    "    \"\"\"\n",
    "\n",
    "    # Add bubbles for each speech segment\n",
    "    for bubble in speech_bubbles:\n",
    "        speaker_class = \"speaker-0\" if bubble[\"speaker\"] == \"SPEAKER_00\" else \"speaker-1\"\n",
    "        text = bubble[\"text\"]\n",
    "        if bubble[\"overlap\"]:\n",
    "            # Highlight overlapping words\n",
    "            overlap_words = bubble[\"overlap\"].split()\n",
    "            for word in overlap_words:\n",
    "                text = text.replace(word, f\"<span class='overlap-word'>{word}</span>\")\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"bubble {speaker_class}\">\n",
    "            <div class=\"text\">{text}</div>\n",
    "            <div class=\"timestamp\">[{bubble[\"start\"]:.2f} - {bubble[\"end\"]:.2f}]</div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    # Close the HTML structure\n",
    "    html_content += \"\"\"\n",
    "        </div>\n",
    "        <script src=\"https://cdn.plyr.io/3.7.8/plyr.polyfilled.js\"></script>\n",
    "        <script>\n",
    "            const player = new Plyr('#player', {\n",
    "                controls: ['play', 'progress', 'current-time', 'duration', 'mute', 'volume']\n",
    "            });\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)\n",
    "\n",
    "    print(f\"HTML file with media player has been generated: {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubbles = create_speech_bubbles_t(test)\n",
    "\n",
    "generate_html_with_media_player_t(bubbles, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubbles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8000/transcribe_audio_bulk\"\n",
    "\n",
    "file_name = 'mmvb3.wav'\n",
    "#'2023-09-15_15-35-23_Resources_89688627131_TRG4D987PP3P3AE4SA59SAUVVK14OK43_pcmu.wav'\n",
    "file_path = f'C:/Users/Alex/whisper_asr_implementation/Drafts/{file_name}'#f'E:/Записи/Мигкредит/1/{file_name}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {}\n",
    "files=[\n",
    "  ('files',(file_name,open(file_path,'rb'),'audio/wav'))\n",
    "]\n",
    "headers = {}\n",
    "\n",
    "transcription = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "print('ASR -- OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://127.0.0.1:8001/diarize_audio_bulk\"\n",
    "payload = {'num_speakers': None}\n",
    "files=[\n",
    "  ('files',(file_name,open(file_path,'rb'),'audio/wav'))\n",
    "]\n",
    "headers = {}\n",
    "segments = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "print('DIARIZATION -- OK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments2 = segments.json()[file_name]['diarization']\n",
    "transcription2 = transcription.json()[file_name]\n",
    "\n",
    "url = \"http://127.0.0.1:8002/process-transcription\"\n",
    "\n",
    "process_payload = {\n",
    "    \"transcription\": transcription2,\n",
    "    \"diarization\": segments2\n",
    "}\n",
    "\n",
    "bubbles = requests.request(\"POST\", url, json=process_payload)\n",
    "print('ALLIGNMENT -- OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://127.0.0.1:8002/generate-html\"\n",
    "\n",
    "bubbles_input2 = bubbles.json()['speech_bubbles']\n",
    "query_params = {\"audio_file_url\": file_path}\n",
    "\n",
    "\n",
    "html = requests.request(\"POST\", url, params=query_params, json=bubbles_input2)\n",
    "print('HTML PREVIEW -- OK')\n",
    "\n",
    "html_content = html.json()['html']\n",
    "\n",
    "with open('./test_preview.html', \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = ['🧑🏾','🧑🏽','🧑🏻','🧑🏿']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
